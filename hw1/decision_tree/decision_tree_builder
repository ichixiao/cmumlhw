'''
comment the code published in http://www.jdxyw.com/?p=2095
copyright@the coder
for learning python & backup

simpler stop condition
'''
from string import split
import math
import operator

def majorityCnt(classlist):
    classcount = {}
    for vote in classlist:
        if vote not in classcount.keys():
            classcount[vote] = 0
        classcount[vote] += 1
        #numbers = {'first': 1, 'second': 2, 'third': 3, 'Fourth': 4}
        #sorted(numbers)  ['Fourth', 'first', 'second', 'third']
        #sorted(numbers.values) [1, 2, 3, 4]
        #sort the key by the value
        #sorted(numbers, key = numbers.__getitem__) ['first', 'second', 'third', 'Fourth']
        #sorted(numbers, key=numbers.__getitem__, reverse=True) ['Fourth', 'third', 'second', 'first']

        #operator.itemgetter(item) return a callable object that fetches item from its operand using the operand's __getitem__() method
        #if multiple items are specified, return a tuple of lookup values
        sortedClassCount = sorted(classcount.iteritems(), key = operator.itemgetter(1), reverse=True)

        return sortedClassCount

    def entropy(dataset):
        n = len(dataset)
        labels = {}
        for record in dataset:
            label = record[-1]
            if label not in labels.keys():
                labels[label] = 0
            labels[label] += 1
            ent = 0.0
            for key in labels.keys():
                prob = float(labels[key])/n
                ent = -prob * math.log(prob, 2)

        return ent

    #split each feature according to their values
    def splitDataset(dataset, nclom, value):
        retDataset = []
        for record in dataset:
            if record[nclom] == value:
                #
                reducedRecord = record[:nclom]
                #
                reducedRecord.extend(record[nclom+1:])
                retDataset.append(reducedRecord)

        return retDataset


    def chooseBestFeatureToSplit(dataset):
        #minus1 because the output is also in the dataset, so the feature number is 1 smaller
        numberFeature = len(dataset[0]) - 1
        baseEntropy = entropy(dataset)
        bestInfoGain = 0.0
        bestFeature = -1
        for i in range(numberFeature):
            featureList = [x[i] for x in dataset]
            #set operation same as unique in R
            uniqueValues = set(featureList)
            newEntropy = 0.0
            for value in uniqueValues:
                subDataset = splitDataset(dataset, i, value)
                prob = len(subDataset)/float(len(dataset))
                newEntropy += prob * entropy(subDataset)
            infoGain = baseEntropy - newEntropy
            if infoGain > bestInfoGain:
                bestInfoGain = infoGain
                bestFeature = i

        return bestFeature

    def buildTree(dataset, labels):
        classlist = [x[-1] for x in dataset]
        if classlist.count(classlist[0]) == len(classlist):
            return classlist[0]
        if len(classlist) == 1:
            return majorityCnt(classlist)

        bestFeature = chooseBestFeatureToSplit(dataset)
        bestFeatureLabel = labels[bestFeature]

        tree = {bestFeatureLabel:{}}
        #not search the whole space every time, but simplify the stop condition
        del(labels[bestFeature])
        featValues = [x[bestFeature] for x in dataset]
        uniqueVals = set(featValues)
        for value in uniqueVals:
            subLabels = labels[:]
            #build the tree recursively, until all labels are added
            tree[bestFeatureLabel][value] = buildTree(splitDataset(dataset, bestFeature, value), subLabels)

        return tree

    def classify(tree, labels, testvec):
        firstStr = tree.keys()[0]
        secondDict = tree[firstStr]
        featIndex = labels.index(firstStr)
        for key in secondDict.keys():
            if testvec[featIndex] == key:
                #check the type
                #__name__ build-in function
                if type(secondDict[key]).__name__ == 'dict':
                    classLabel = classify(secondDict[key],labels,testvec)
                else:
                    classLabel = secondDict[key]
            #error and exception
            try:
                return classLabel
            except:
                return 1
